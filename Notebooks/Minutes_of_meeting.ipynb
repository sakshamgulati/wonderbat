{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "F1N_a7zO2Bx8"
      },
      "outputs": [],
      "source": [
        "#whisper\n",
        "class whisper_transcribe():\n",
        "  def __init__(self,file_location):\n",
        "    self.file_location=self.file_location\n",
        "\n",
        "    # --form self.file_location=@openai.mp3 \\\n",
        "    # --form model=whisper-1 \\\n",
        "    # --form response_format=text\n",
        "\n",
        "\n",
        "  def transcribe(file_location):\n",
        "    \n",
        "    audio_file= open(file_location, \"rb\")\n",
        "    transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n"
      ],
      "metadata": {
        "id": "Z1yX497a2a_f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Gc-W8Nx7wyC",
        "outputId": "c9b1bddf-8ddd-4c33-a556-c5159f1b0af6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alOUxmVa3WZy",
        "outputId": "4077bbf0-6a76-4f5d-b07b-24933b42ea66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.2-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 KB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.9/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.2 yarl-1.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import os\n",
        "\n",
        "import openai\n",
        "\n",
        "import re\n",
        "from os.path import splitext, exists\n",
        "def clean_webvtt(filepath: str) -> str:\n",
        "    \"\"\"Clean up the content of a subtitle file (vtt) to a string\n",
        "\n",
        "    Args:\n",
        "        filepath (str): path to vtt file\n",
        "\n",
        "    Returns:\n",
        "        str: clean content\n",
        "    \"\"\"\n",
        "    # read file content\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as fp:\n",
        "        content = fp.read()\n",
        "\n",
        "    # remove header & empty lines\n",
        "    lines = [line.strip() for line in content.split(\"\\n\") if line.strip()]\n",
        "    lines = lines[1:] if lines[0].upper() == \"WEBVTT\" else lines\n",
        "\n",
        "    # remove indexes\n",
        "    lines = [lines[i] for i in range(len(lines)) if not lines[i].isdigit()]\n",
        "\n",
        "    # remove tcode\n",
        "    #pattern = re.compile(r'^[0-9:.]{12} --> [0-9:.]{12}')\n",
        "    pattern = r'[a-f\\d]{8}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{4}-[a-f\\d]{12}\\/\\d+-\\d'\n",
        "    lines = [lines[i] for i in range(len(lines))\n",
        "             if not re.match(pattern, lines[i])]\n",
        "\n",
        "    # remove timestamps\n",
        "    pattern = r\"^\\d{2}:\\d{2}:\\d{2}.\\d{3}.*\\d{2}:\\d{2}:\\d{2}.\\d{3}$\"\n",
        "    lines = [lines[i] for i in range(len(lines))\n",
        "             if not re.match(pattern, lines[i])]\n",
        "\n",
        "    content = \" \".join(lines)\n",
        "\n",
        "    # remove duplicate spaces\n",
        "    pattern = r\"\\s+\"\n",
        "    content = re.sub(pattern, r\" \", content)\n",
        "\n",
        "    # add space after punctuation marks if it doesn't exist\n",
        "    pattern = r\"([\\.!?])(\\w)\"\n",
        "    content = re.sub(pattern, r\"\\1 \\2\", content)\n",
        "\n",
        "    return content\n",
        "\n",
        "\n",
        "def vtt_to_clean_file(file_in: str, file_out=None, **kwargs) -> str:\n",
        "    \"\"\"Save clean content of a subtitle file to text file\n",
        "\n",
        "    Args:\n",
        "        file_in (str): path to vtt file\n",
        "        file_out (None, optional): path to text file\n",
        "        **kwargs (optional): arguments for other parameters\n",
        "            - no_message (bool): do not show message of result.\n",
        "                                 Default is False\n",
        "\n",
        "    Returns:\n",
        "        str: path to text file\n",
        "    \"\"\"\n",
        "    # set default values\n",
        "    no_message = kwargs.get(\"no_message\", False)\n",
        "    if not file_out:\n",
        "        filename = splitext(file_in)[0]\n",
        "        file_out = \"%s.txt\" % filename\n",
        "        i = 0\n",
        "        while exists(file_out):\n",
        "            i += 1\n",
        "            file_out = \"%s_%s.txt\" % (filename, i)\n",
        "\n",
        "    content = clean_webvtt(file_in)\n",
        "    with open(file_out, \"w+\", encoding=\"utf-8\") as fp:\n",
        "        fp.write(content)\n",
        "    if not no_message:\n",
        "        print(\"clean content is written to file: %s\" % file_out)\n",
        "\n",
        "    return file_out"
      ],
      "metadata": {
        "id": "xjlmXQJd3W2H"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "def count_tokens(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "    tokens = word_tokenize(text)\n",
        "    return len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bH0L45gt3YAo",
        "outputId": "a9373ea6-969f-467b-bab0-888545118c05"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/MyDrive/minutes_of_meetings/ANC vs Wysdom OC Deliverable Discussion_2023-03-15.vtt\"\n",
        "file=vtt_to_clean_file(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqTKTnF47FMb",
        "outputId": "b2f26025-3f7c-4b82-9166-f6630ab227e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "clean content is written to file: /content/drive/MyDrive/minutes_of_meetings/ANC vs Wysdom OC Deliverable Discussion_2023-03-15_2.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nNeKXKqL4aMs",
        "outputId": "681d1399-3ecd-4d14-9600-eba7c502917b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/minutes_of_meetings/ANC vs Wysdom OC Deliverable Discussion_2023-03-15_2.txt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "token_count = count_tokens(file)\n",
        "print(f\"Number of tokens: {token_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGb0QfG84PIG",
        "outputId": "3f7df38c-505c-4331-8d5b-326e01668e14"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 6801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def break_up_file(tokens, chunk_size, overlap_size):\n",
        "    if len(tokens) <= chunk_size:\n",
        "        yield tokens\n",
        "    else:\n",
        "        chunk = tokens[:chunk_size]\n",
        "        yield chunk\n",
        "        yield from break_up_file(tokens[chunk_size-overlap_size:], chunk_size, overlap_size)\n",
        "\n",
        "def break_up_file_to_chunks(filename, chunk_size=2000, overlap_size=100):\n",
        "    with open(filename, 'r') as f:\n",
        "        text = f.read()\n",
        "    tokens = word_tokenize(text)\n",
        "    return list(break_up_file(tokens, chunk_size, overlap_size))"
      ],
      "metadata": {
        "id": "CEBEy9fT8l-J"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chunks = break_up_file_to_chunks(filename)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Chunk {i}: {len(chunk)} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTsv5Y4T89-P",
        "outputId": "1f145a4c-002f-4e56-8c0b-db587e669e0f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 0: 2000 tokens\n",
            "Chunk 1: 2000 tokens\n",
            "Chunk 2: 2000 tokens\n",
            "Chunk 3: 2000 tokens\n",
            "Chunk 4: 134 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_prompt_text(tokenized_text):\n",
        "    prompt_text = \" \".join(tokenized_text)\n",
        "    prompt_text = prompt_text.replace(\" 's\", \"'s\")\n",
        "    return prompt_text"
      ],
      "metadata": {
        "id": "hU92ihnv9CLA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_response = []\n",
        "import openai\n",
        "chunks = break_up_file_to_chunks(filename)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    prompt_request = \"Summarize this meeting transcript: \" + convert_to_prompt_text(chunks[i])\n",
        "    messages = [{\"role\": \"system\", \"content\": \"This is text summarization.\"}]    \n",
        "    messages.append({\"role\": \"user\", \"content\": prompt_request})\n",
        "    response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=messages,\n",
        "            temperature=.5,\n",
        "            max_tokens=500,\n",
        "            top_p=1,\n",
        "            frequency_penalty=0,\n",
        "            presence_penalty=0\n",
        "    )\n",
        "    \n",
        "    prompt_response.append(response[\"choices\"][0][\"message\"]['content'].strip())"
      ],
      "metadata": {
        "id": "p7msrwxz9GJC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# meeting_summary = prompt_response[\"choices\"][0][\"text\"].strip()\n",
        "print(prompt_response)\n",
        "# prompt_response.append(response[\"choices\"][0][\"text\"].strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTjWeGRf9Plj",
        "outputId": "8d8f0f59-23cd-48dd-82e2-f691d0b8e52a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The meeting is discussing the technical and business perspectives of creating a dashboard tool for topic modeling. The tool would be used to monitor top click call drivers and intent drivers to channels, review conversations, and identify friction points between channels. The goal is to replace the wisdom.ai tool and create a consistent view of all conversations. The team also discusses the possibility of having one topic modeling for all channels instead of separate ones. The tool would be used for improving customer experience and making improvements quickly.', 'The meeting discussed the need to understand the intents of customers who use the virtual system and whether their issues are related to billing or something else. They also talked about the importance of measuring containment, which is the opposite of customers contacting support. The current reporting only shows how many conversations there were and how many contacted support, but they want to be able to understand what the conversations were about and what customers were contacting support for. They also discussed the possibility of automating certain topics to prevent customers from needing to contact support.', 'The meeting discusses the need to correct a billing error and the use of containment matrix to enable self-serve for certain customer intents. The current containment metric for VA is around 30%, and the team is looking at improving it through personalized solutions in VA. The discussion also covers data sources, including the event hub for VA and chat and call transcripts. The meeting ends with a request for access to data sources.', 'The meeting was focused on setting up a system to better understand the journey of virtual assistants for Rogers. The team discussed the different data sources they will need access to, such as VA and Boldchat, and the importance of understanding specific topics related to billing. They also discussed the need for more detailed categorization of intents to better analyze data. The meeting ended with a request to submit an intake form for the project.', 'The meeting participants express gratitude and say their goodbyes. Saksham asks for something to be done and Kelly says they will see. Tom thanks everyone and asks if they can leave. Kelly thanks everyone and wishes them a good night. The meeting ends with Saksham saying goodbye and Speaker 1 thanking everyone.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F0d3-5hXQRK-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}